{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 26401,
     "status": "ok",
     "timestamp": 1596922317612,
     "user": {
      "displayName": "Mahdiyar Shahbazi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj7XCVw5ppYcG3Zk2LvoE3xC3Czn3cNS-mgiV9R=s64",
      "userId": "02283675187783311712"
     },
     "user_tz": -270
    },
    "id": "RRpXksge8M8u",
    "outputId": "4d1bd43d-80c0-473b-9292-f06bd168e017"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8415,
     "status": "ok",
     "timestamp": 1596922351833,
     "user": {
      "displayName": "Mahdiyar Shahbazi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj7XCVw5ppYcG3Zk2LvoE3xC3Czn3cNS-mgiV9R=s64",
      "userId": "02283675187783311712"
     },
     "user_tz": -270
    },
    "id": "g-RpDXLi8TeE",
    "outputId": "6289a936-6d6d-474f-f81a-11093c9932fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/.shortcut-targets-by-id/15CQlDgewua6NDmwLKRc25XqjLXZn08bL/Sanity Check for Linear CKA\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/content/drive/My Drive/Colab Notebooks/neural_network_model')\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1342,
     "status": "ok",
     "timestamp": 1596922365938,
     "user": {
      "displayName": "Mahdiyar Shahbazi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj7XCVw5ppYcG3Zk2LvoE3xC3Czn3cNS-mgiV9R=s64",
      "userId": "02283675187783311712"
     },
     "user_tz": -270
    },
    "id": "SGlOsTll8Utr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %tensorflow_version 1.x\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "# from tensorflow.keras.datasets import cifar10\n",
    "from keras import datasets\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import seaborn as sns\n",
    "from itertools import permutations\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VohVaRwQmeRh"
   },
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1443,
     "status": "ok",
     "timestamp": 1596922370238,
     "user": {
      "displayName": "Mahdiyar Shahbazi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj7XCVw5ppYcG3Zk2LvoE3xC3Czn3cNS-mgiV9R=s64",
      "userId": "02283675187783311712"
     },
     "user_tz": -270
    },
    "id": "PbLLpqXK8M81"
   },
   "outputs": [],
   "source": [
    "cifar_model = 10\n",
    "num_classes = cifar_model\n",
    "img_size = 32\n",
    "n_trial = 10  # number of models with different initialization\n",
    "n_sample = 20\n",
    "CH0 = 16  # number of 1st layer channels\n",
    "epoch_size = 26\n",
    "batch_size = 128       \n",
    "n_step = 50\n",
    "\n",
    "log_path = 'logs/model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_javIJIZ_b5A"
   },
   "outputs": [],
   "source": [
    "if os.path.exists(log_path):\n",
    "    raise Exception(\"Report path exists!\")\n",
    "else:\n",
    "    os.makedirs(log_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BDfwPd5foWDY"
   },
   "source": [
    "# Load CIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13943,
     "status": "ok",
     "timestamp": 1596922400868,
     "user": {
      "displayName": "Mahdiyar Shahbazi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj7XCVw5ppYcG3Zk2LvoE3xC3Czn3cNS-mgiV9R=s64",
      "userId": "02283675187783311712"
     },
     "user_tz": -270
    },
    "id": "zn6WNxeeC0bQ",
    "outputId": "70a30f14-f1a7-4493-d701-8776d71af730"
   },
   "outputs": [],
   "source": [
    "if cifar_model == 10:\n",
    "    (x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()#cifar10.load_data()\n",
    "else:\n",
    "    raise Exception(\"Invalid model!\")\n",
    "\n",
    "x_mean = np.mean(x_train)\n",
    "x_std = np.std(x_train)\n",
    "eps = 0.0001\n",
    "\n",
    "train_size = len(x_train)\n",
    "test_size = len(x_test)\n",
    "\n",
    "y_train_one_hot = to_one_hot(np.squeeze(y_train), num_classes)\n",
    "y_test_one_hot = to_one_hot(np.squeeze(y_test), num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "19iFgVLIYSph"
   },
   "source": [
    "# Train Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IQ4nNXsSYR6O"
   },
   "outputs": [],
   "source": [
    "for trial in range(n_trial):\n",
    "  # Load model\n",
    "  tf.reset_default_graph()\n",
    "  layers, pred, X, Y, IS_TRAINING, LR, train_opt, Loss = get_model(img_size, CH0, num_classes)\n",
    "\n",
    "  config = tf.ConfigProto()\n",
    "  config.gpu_options.allow_growth = True\n",
    "  sess = tf.Session(config = config)\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  saver = tf.train.Saver()\n",
    "\n",
    "  t0 = time.time()\n",
    "\n",
    "  epoch = 0\n",
    "  step = 0\n",
    "  n_batchs = train_size // batch_size\n",
    "  lr = 0.1\n",
    "\n",
    "  while epoch < epoch_size:\n",
    "      \n",
    "    if epoch == 18:\n",
    "        lr *= 0.1\n",
    "\n",
    "    if epoch == 22:\n",
    "        lr *= 0.1\n",
    "        \n",
    "    inds = np.random.permutation(train_size)\n",
    "    \n",
    "    for i_batch_train in range(n_batchs):\n",
    "      batch_ind_train = inds[i_batch_train*batch_size: (i_batch_train+1)*batch_size]\n",
    "\n",
    "      batch_x = (x_train[batch_ind_train].copy() - x_mean) / (x_std + eps)\n",
    "      batch_x = np.concatenate((random_crop(batch_x), batch_x), axis = 0)\n",
    "      batch_y = y_train_one_hot[batch_ind_train].copy()\n",
    "      batch_y = np.concatenate((batch_y, batch_y), axis = 0)\n",
    "              \n",
    "      feed = {X: batch_x, Y: batch_y, IS_TRAINING:True, LR: lr}\n",
    "      sess.run(train_opt, feed)\n",
    "      step += 1\n",
    "\n",
    "      if step % n_step == 0:\n",
    "        output = []\n",
    "        loss_ce = []\n",
    "        for i_batch_test in range(test_size // batch_size):\n",
    "          batch_x = (x_test[i_batch_test*batch_size: (i_batch_test+1)*batch_size] - x_mean) / (x_std + eps)\n",
    "          batch_y = y_test_one_hot[i_batch_test*batch_size: (i_batch_test+1)*batch_size]\n",
    "          output_, loss_ = sess.run([pred, Loss], feed_dict={X: batch_x, Y: batch_y, IS_TRAINING:False})\n",
    "          output.extend(output_)\n",
    "          loss_ce.append(loss_)\n",
    "        hit = np.argmax(output,1) - np.argmax(y_test_one_hot[:len(output)],1)\n",
    "        acc_test = hit[hit == 0.].shape[0] / len(hit)\n",
    "\n",
    "        loss_test = np.mean(loss_ce)\n",
    "\n",
    "      if step % (10*n_step) == 0:\n",
    "        print('trial: %d, epoch: %.2d, loss test: %.3f, accuray test: %.3f'%(trial, epoch, loss_test, acc_test))\n",
    "            \n",
    "    epoch += 1\n",
    "\n",
    "  trial_path = log_path + 'trial_' + str(trial+1) + '/'\n",
    "\n",
    "  if os.path.exists(trial_path):\n",
    "    raise Exception(\"Report path exists!\")\n",
    "  else:\n",
    "    os.makedirs(trial_path) \n",
    "\n",
    "  print(\"trial: \" + str(trial) + \" Epoch: \" + str(epoch) + \", elapsed time: %d min\"% ((time.time() - t0)/60))       \n",
    "  saver.save(sess, save_path=trial_path + 'trial_' + str(trial+1) + '.ckpt') \n",
    "  sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dw05ZkWnoofY"
   },
   "source": [
    "# Sampling from images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2072,
     "status": "ok",
     "timestamp": 1596922406074,
     "user": {
      "displayName": "Mahdiyar Shahbazi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj7XCVw5ppYcG3Zk2LvoE3xC3Czn3cNS-mgiV9R=s64",
      "userId": "02283675187783311712"
     },
     "user_tz": -270
    },
    "id": "aywB4LEC8M9B"
   },
   "outputs": [],
   "source": [
    "x_train_class =  []\n",
    "for i_class in range(num_classes):\n",
    "  x_train_class += [np.random.permutation(x_train[y_train[:, 0] == i_class])[:n_sample]]\n",
    "\n",
    "x_mean = np.mean(x_train)\n",
    "x_std = np.std(x_train)\n",
    "eps = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1402,
     "status": "ok",
     "timestamp": 1596922408026,
     "user": {
      "displayName": "Mahdiyar Shahbazi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj7XCVw5ppYcG3Zk2LvoE3xC3Czn3cNS-mgiV9R=s64",
      "userId": "02283675187783311712"
     },
     "user_tz": -270
    },
    "id": "c4etJh2r8M9L"
   },
   "outputs": [],
   "source": [
    "y_train_sampled = to_one_hot(np.array(range(num_classes)), num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'layers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-1-4e3e77279424>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mlayers\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'layers' is not defined"
     ]
    }
   ],
   "source": [
    "layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "czqLJUlxbQbI"
   },
   "source": [
    "# Big Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qo66BaK_8M9P"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mahdiyar/Desktop/Sanity Check for Linear CKA/model.py:8: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/mahdiyar/Desktop/Sanity Check for Linear CKA/model.py:11: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/mahdiyar/Desktop/Sanity Check for Linear CKA/model.py:77: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/mahdiyar/Desktop/Sanity Check for Linear CKA/model.py:72: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "WARNING:tensorflow:From /home/mahdiyar/.local/lib/python3.6/site-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /home/mahdiyar/Desktop/Sanity Check for Linear CKA/model.py:36: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /home/mahdiyar/Desktop/Sanity Check for Linear CKA/model.py:40: The name tf.losses.get_regularization_loss is deprecated. Please use tf.compat.v1.losses.get_regularization_loss instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/mahdiyar/Desktop/Sanity Check for Linear CKA/model.py:42: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from logs/model/trial_1/trial_1.ckpt\n",
      "The accuracy of the model on the test dataset is: 84.98\n",
      "INFO:tensorflow:Restoring parameters from logs/model/trial_2/trial_2.ckpt\n",
      "The accuracy of the model on the test dataset is: 84.86\n",
      "INFO:tensorflow:Restoring parameters from logs/model/trial_3/trial_3.ckpt\n",
      "The accuracy of the model on the test dataset is: 85.47\n",
      "INFO:tensorflow:Restoring parameters from logs/model/trial_4/trial_4.ckpt\n",
      "The accuracy of the model on the test dataset is: 85.46\n",
      "INFO:tensorflow:Restoring parameters from logs/model/trial_5/trial_5.ckpt\n",
      "The accuracy of the model on the test dataset is: 84.96\n",
      "INFO:tensorflow:Restoring parameters from logs/model/trial_6/trial_6.ckpt\n",
      "The accuracy of the model on the test dataset is: 85.05\n",
      "INFO:tensorflow:Restoring parameters from logs/model/trial_7/trial_7.ckpt\n",
      "The accuracy of the model on the test dataset is: 85.06\n",
      "INFO:tensorflow:Restoring parameters from logs/model/trial_8/trial_8.ckpt\n",
      "The accuracy of the model on the test dataset is: 85.37\n",
      "INFO:tensorflow:Restoring parameters from logs/model/trial_9/trial_9.ckpt\n",
      "The accuracy of the model on the test dataset is: 84.94\n",
      "INFO:tensorflow:Restoring parameters from logs/model/trial_10/trial_10.ckpt\n",
      "The accuracy of the model on the test dataset is: 84.63\n"
     ]
    }
   ],
   "source": [
    "# Init d_list: \n",
    "tf.reset_default_graph()\n",
    "layers, _, X, Y, IS_TRAINING, _, _, _ = get_model(img_size, CH0, num_classes)\n",
    "\n",
    "n_layer = len(layers)\n",
    "d_list = []\n",
    "for i_layer in range(n_layer):\n",
    "  n_w = np.prod(layers[i_layer].shape[1:])\n",
    "  d_list += [np.zeros((num_classes, n_w, n_sample, n_trial))]\n",
    "\n",
    "# Loop on models\n",
    "for trial in range(n_trial):\n",
    "  # Load model\n",
    "  tf.reset_default_graph()\n",
    "  layers, pred, X, Y, IS_TRAINING, _, _, _ = get_model(img_size, CH0, num_classes)\n",
    "\n",
    "  config = tf.ConfigProto()\n",
    "  config.gpu_options.allow_growth = True\n",
    "  sess = tf.Session(config = config)\n",
    "  saver = tf.train.Saver()\n",
    "  trial_path = trial_path = log_path + 'trial_' + str(trial+1) + '/'\n",
    "  saver.restore(sess, save_path=trial_path + 'trial_' + str(trial + 1) + '.ckpt')\n",
    "  # end load model\n",
    "\n",
    "  # The accuracy of the model on the test dataset\n",
    "  output = []\n",
    "  for i_batch_test in range(test_size // batch_size):\n",
    "      batch_x = (x_test[i_batch_test*batch_size: (i_batch_test+1)*batch_size] - x_mean) / (x_std + eps)\n",
    "      batch_y = y_test_one_hot[i_batch_test*batch_size: (i_batch_test+1)*batch_size]\n",
    "      output_ = sess.run(pred, feed_dict={X: batch_x, Y: batch_y, IS_TRAINING:False})\n",
    "      output.extend(output_)\n",
    "  hit = np.argmax(output,1) - np.argmax(y_test_one_hot[:len(output)],1)\n",
    "  acc_t = hit[hit == 0.].shape[0] / len(hit)\n",
    "\n",
    "  print('The accuracy of the model on the test dataset is: %.2f'%(100*acc_t))\n",
    "\n",
    "  # Loop on samples of images \n",
    "  for i_sample in range(n_sample):\n",
    "      x_train_sampled = np.zeros((num_classes, img_size, img_size, 3))\n",
    "\n",
    "      for i_class in range(num_classes):\n",
    "        x_train_sampled[i_class] = x_train_class[i_class][i_sample]\n",
    "\n",
    "      x_train_sampled_norm = (x_train_sampled - x_mean)/(x_std + eps)\n",
    "\n",
    "      # Get model's layers outputs\n",
    "      u_list = sess.run(layers[:n_layer], \n",
    "                        feed_dict={X: x_train_sampled_norm, Y: y_train_sampled, IS_TRAINING:False})\n",
    "\n",
    "      for i_layer in range(n_layer): \n",
    "        d_list[i_layer][:, :, i_sample, trial] = np.reshape(u_list[i_layer], (10, -1))\n",
    "        \n",
    "  sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 64, 20, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "sio.savemat('d_list.mat', {'l1':d_list[0],'l2':d_list[1],'l3':d_list[2],'l4':d_list[3],'l5':d_list[4]\n",
    "                          ,'l6':d_list[5],'l7':d_list[6],'l8':d_list[7],'l9':d_list[8]})\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}